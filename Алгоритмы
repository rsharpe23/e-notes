СТРУКТУРЫ ДАННЫХ И УСТРОЙСТВО СТЕКА

Структуры данных и алгоритмы нераздельно связаны между собой.
Правильно подобранная структура данных влияет на производимость алгоритма.

Есть 7 основных структур данных:
1) Массив.
2) Связанный список.
3) Стек.
4) Очередь.
5) Множество.
6) Хэш-таблица.
7) Граф.

Все они в основном абстрактны и в разных языках могут реализовываться по своему, 
а также иметь отличающиеся имена.

Массив и связанный список - это две фундаментальные структуры данных. 
На них построены все остальные.

Массивы хороши, когда нужно получать данные, но плохи, когда нужно их вставлять/удалять.
Связанные списки хороши, когда нужно получать/вставлять/удалять данные в начало или в
конец, но плохи, когда нужно это делать в середину.

Массив - это непрерывный участок памяти, хранящий группу элементов. Массивы статичны, 
т.е. после их создания новые элементы добавлять не получится.

Это из-за того, что группа элементов должна храниться в памяти непрерывно, и если
попытаться её расширить, то может получиться так, что адрес для нового элемента, 
уже занят другой переменной.

Для массива можно зарезервировать дополнительные места при создании, а при вставке
новых элементом просто сдвигать предыдущие. Но в этом случае будет неоправданный 
расход памяти, т.к. новых элементов может и не быть.

Все операции, кроме чтения данных, у массива отсутсвуют. А если таковы и есть, 
то это означает, что под капотом создается новый массив, в другом участке памяти.
Так работают динамические массивы (vector и т.п.).

Динамические массивы не пересоздаются каждый раз, при добавлении нового элемента, а
берут небольшой запас вместимости напрёд, расширяясь только через каждые n-добавлений.

Бывает так, что при наличии места в памяти, массив все равно не получается создать. 
Это из-за того, что там нет ни одного свободного участка для его размерности.
Посему размер массива может быть ограниченным.

Связанный список - это цепочка объектов, где каждый объект хранит ссылку на следующего
соседа. Внутри связанного списока есть доступ только к первому и последнему объектам.
Его размер ограничен кол-вом самой памяти, т.к. объекты находятся произвольно.

Связанный список реализуется примерно так:
...
bool moveNext(int& value)
{
    if (current) {
        value = current->value;
        current = current->next;
        return true;
    }

    return false;
}

void reset() { current = first; }

void addValue(Node* node)
{
    if (last) {
        last->next = node;
        last = last->next;
    } else {
        last = node;
        first = last;
    }
}

Этот алгоритм будет работать только в C++. Здесь основная мысль в том, чтобы сначала
задать next, а затем этот next задать как текущий last, при этом ссылка на предыдущий
last просто потеряется. Именно это позволяет списку быть быстрым на вставку/удаление 
в начало и в конец - O(1).

В других языках, где есть сборщик мусора, такая реализация не прокатит, поскольку
сборщик мусора будет тупо удалять все предыдущие last'ы, которые теряют ссылку. 

Там реализация связанного списка будет иметь скорость O(n), т.к. для обращения к
последнему last'у нужно будет пройтись по всему списку циклом.

В двусвязанном списке каждый объект хранит ссылку как на следующего, 
так и на предыдущего соседа в цепочке.

Стек - это список, работающий по принципу LIFO (последний пришел, первый ушел).
Очередь - это список, работающий по принципу FIFO (первый пришел, первый ушел).
Множество - это список, содержащий только уникальные значения (Set).

В оперативной памяти также есть стек. Там он представлен в виде специальной области,
куда записываются переменные. У каждой переменной есть адрес в шестнадцатиричном виде.
Эта область манипулирует данные автоматически, по такому же самому принципу - LIFO.

Помимо этого существует ещё и стек вызовов ф-ций (call stack). 
Когда вызывается ф-ция, в стек вызовов записывается специальный фрейм, содержащий 
адрес её возврата, параметры, локальные переменные и другую служебную инфу. 
Адрес возврата - это адрес следующей инструкции после вызова ф-ции.

Когда внутри одной ф-ции вызывается другая ф-ция, то первая приостанавливает свое
выполнение и ждет когда вернется исполнение из второй. Когда ф-ция рекурсивно 
вызывает саму себя, то происходит тоже самое. 

При рекурсивных вызовах, стек сначала раскручивается, заполняясь промежуточными
фреймами от каждого вызова, а затем скручивается, передавая итоговый результат 
(базовый случай) по цепочке назад, до самого первого вызова. Т.е. по сути 
работа рекурсии напоминает работу стека.

Рекурсии часто применяются в алгоритмах. Но у них есть и определенный недостаток.
Если рекурсивно обрабатывать большую коллекцию, то это может привести к переполнению
стэка из-за большого кол-ва фреймов. Посему, большие коллекции лучше обрабат. циклом.

Множества хорошо подходят для того, чтобы быстро выполнять такие операции как:
объединение, пересечение, разность (между списками данных).

Хэш-таблица - это ассоциативный массив (Map). Он устроен на основе обычного массива и 
хеш-функции. Хеш-функция работает по алгоритму SHA. Проще говоря, она принимает строку
и возвращает число. Вместо строки могут быть любые данные, например объект.

Когда в хеш-таблицу заносится значение, то выполняется следующее:
1) Ключ, с помощью хеш-функции, преобразовывается в индекс.
2) Значение добавляется в массив под полученным индексом.

Примитивный алгоритм работы хеш-функции: 
1) Для каждой буквы берется соответсвующее число.
2) Числа суммируются, а результат делится с остатком на размер массива.

Пример для строки "bag", с размером массива 10: (3 + 2 + 17) % 10 = 2.

Массив внутри хеш-таблицы работает примерно также, как и динамический, т.е. сначала 
он имеет какую-то базовую вместимость, например 10, затем, по мере заполения элементов,
он увеличивается ещё на 10 и т.д. Это означает, что при расширенияи хеш-таблицы, старые
значения будут получать новые индексы, и не смогут претендовать на одну и ту же позицию.

Но на практике коллизии все равно будут возникать (даже при использовании сложных
алгоритмов SHA) и в таких случаях для повторяющихся индексов обычно используют
связанный список. Чем плотнее заполняется массив, тем больше вероятность появления
коллизий. Поэтому массив принято расширять на 75% заполнения.

Граф - это набор узлов и ребер. Ребра определяют отношения между узлами, а сами узлы
это данные. Графы используются для моделирования связей между разными объектами.

Узлы, указывающие друг на друга, называются соседями. Ближайшие соседи - это связи
первого уровня, соседи соседей - связи второго уровня и т.д.

В направленном графе отношения идут только в одну сторону: A <- B (B зависит от A).
В ненаправленном графе отношения идут в обе стороны: A -- B (B от A, A от B).

Граф, в котором нет ребер указывающих в обратном направлении, называется "деревом".
Дерево: A -> B, B -> C, B -> D. Не дерево: A -> B, B -> C, B -> D, C -> A.

Граф обычно представляют в виде хеш-таблицы, где ключ - это узел, а значение - массив
из соседних узлов. Если граф числовой, то вместо хеш-таблицы можно использовать 
массив массивов. В этом случае узлом будет индекс подмассива.

Это называется "список смежности":
...
0 = { 1 }         0 → 1 → 3
1 = { 2, 3 }        ↖ ↓ ↗
2 = { 0, 3 }          2
3 = {}

Но бывает так, что вместо хеш-таблицы берут двумерный массив ребер:
...
{ 0, 1 }
{ 1, 2 }
{ 1, 3 }
{ 2, 0 }
{ 2, 3 }

В этом случае, его надо доп. преобразовать в список смежности:
...
vector<vector<int>> graph(4);  -->  длина равняется кол-ву узлов в графе

for (int i = 0; i < graph.size(); i++) {

    int a = data[i][0];  -->  a, b это случайные индексы
    int b = data[i][1];       на промежутке от 0 до 4

    graph[a].push_back(b);  -->  для ненаправленного графа нужно записать еще
}                                graph[b].push_back(a);

Невзвешанный граф иногда можно представить в виде массива битов, где номер строки
означает узел от которого идет стрелка, а номер колонки - узел, к которому она идет.

  A B C
A|0 1 0
B|1 0 1
C|1 0 0

A -> B, B -> A, B -> C, C -> A

Вместо массива битом может быть двумерный массив из нулей и единиц. 
Это называется "матрица смежности". 

Взвешанный граф - этот тот, чьи ребра имеют вес (время пути). Такие графы обычно
применяются в алгоритмах, где нужно находить кратчайший путь по времени.

Двумерный массив для взвешанного графа, имеет еще одну колонку "вес":
...
{ 0, 1, 7 }
{ 1, 2, 4 }
{ 1, 3, 15 }
{ 2, 0, 1 }
{ 2, 3, 9 }

Если одна вершина графа ведет к другой, другая к третьей, а третья к первой, то
получается цикл. Кроме того, вершины ненаправленного графа также приводя к циклу, 
т.к. всегда ссылаются друг на друга: A -- B. Это стоит учитывать в алгоритмах.

-----------------------

БИНАРНЫЙ ПОИСК И О-БОЛЬШОЕ

Это алгоритм поиска неизвестного числа из последовательности. Работает он по принципу
сокращения зоны поиска вдвое. Допустим есть последовательность чисел от 1 до 100 и 
кто-то загадал число 67. Чтобы его найти максимально быстро, делаем следующее:

1) Берем число 50 и сравниваем его с искомым. Оно меньше, а значит зона поиска
сокращается к последовательности, от 50 до 100.
2) Далее берем 75. Оно больше. Значит зона сократится снова и бедет 50-75.
3) Берем 62 (62.5). Оно меньше. Значит зона станет 62-75.
4) Берем 68. Теперь зона поиска 62-68.
5) Берем 65. Теперь зона поиска 65-68.
6) Берем 66. Теперь зона поиска 66-68.
7) Берем 67 и находим результат.

Бинарный поиск всегда выполняется за log2(n) шагов (логарифм по основанию 2). Это
значит, что для поиска числа 67 было затрачено всего 7 шагов, тогда линейный поиск (n)
(обычный перебор цифр по порядку) занял бы 67 шагол.

Логарифм по смыслу означает поиск степени числа (обратное возведению в степень),
например: log10(100) = 2, т.е. 10 ^ 2 = 100. Соответсвенно log2(100) = 6.644 -> 7.

О-большое описывает скорость выполнения алгоритма. Оно не дает конкертных цифр, а лишь
помогает понять, как быстро будет работать алгоритм при увеличении числа итераций.

О-большое дает самый пессиместичный результат (т.е. для последовательности из 100
чисел, при линейном поиске, результат будет 100, т.к. теоритически вместо 67 могло бы
быть и число 100, тогда для его определения понадобились все 100 шагов).

О-большое записывается так:
1) Для алгоритма линейного поиска - O(n).
2) Для бинарного поиска - O(log2(n)).

В скобках указывается кол-во шагов, выполняемых алгоритмом для получения результата.
Т.е. по сути О-большое это и есть то самое кол-во шагов.

О-большое в основном имеет 5 разновидностей:
1) O(log n) - логарифмическое время; быстрые алгоритмы; 
2) O(n) - линейное время; простые алгоритмы;
3) O(n * log n) - быстрые алгоритмы сортировки (и возможно, что-то ещё);
4) O(n ^ 2) - медленные алгоритмы сортировки (сортировка выбором);
5) O(n!) - очень медленные алгоритмы (например когда нужно найти все возможные
комбинации чисел, в последовательности из n-элементов).

В списке выше, и далее в разделах, запись "log n" обозначает log2(n).
А все примеры будут приведены на языке C++.

Бинарный поиск хорошо подходит для поиска элементов в массиве. При этом элементы могут
быть произвольными числами (или буквами), главное чтобы они были отсортированы.
Например так: 1, 3, 8, 17, 23, 45.

Реализация бинарного поиска:
...
int binarySearch(int* arr, int length, int item)
{
    int low = 0;
    int high = length - 1;

    while (low <= high) {
        int mid = (low + high) / 2;
        int guess = *(arr + mid);

        if (item == guess) {
            return mid;
        }

        if (item > guess) {
            low = mid + 1;
        } else {
            high = mid - 1;
        }
    }

    return -1;
}

-----------------------

МЕДЛЕННЫЕ И БЫСТРЫЕ СОРТИРОВКИ

Сортировка пузырьком: О(n ^ 2). Учебная. Основная идея, это выдавливать самое большое
число в конец при каждой итерации i, сокращая при этом итерации j.
...
void bubbleSort(int* arr, int length)
{
    for (int i = 0; i < length; i++) {
        for (int j = 0; j < length - (i + 1); j++) {
            if (arr[j] > arr[j + 1]) {
                int temp = arr[j];
                arr[j] = arr[j + 1];
                arr[j + 1] = temp;
            }
        }
    }
}

Сортировка выбором: O(n ^ 2). Здесь наоборот, нужно находить минимальное число 
и сдвигать его в начало массива, если оно на другой позиции.
...
void selectionSort(int* begin, int* end)
{
    for (int* it = begin; it != end; it++) {
        int* min = min_element(it, end);
        if (it != min) {
            int temp = *it;
            *it = *min;
            *min = temp;
        }
    }
}

Быстрая сортировка Хоара: O(n * log n). Эффективна на больших массивах. 
Для маленьких можно использовать сортировку выбором.

Основная идея, это выбрать опорный элемент и разбить массив на два подмассива. 
В первый вынести все числа, меньше опорного. Во второй, все, что больше. После этого,
каждый из подмассивов рекурсивно разбить еще на два и т.д, пока не получится множество
отсортированных подмассиовов. В конце все подмассивы склеить в один.

В качестве опорного элемента обычно выбирают либо случайное число из списка, либо
стоящее посередине, Это необходимо, чтобы не попасть на самое большое/маленькое число.

Если опорный элемент будет самым большим или самым маленьким числом, то алгоритм будет
выполнятся за O(n ^ 2), но вероятность такого случая мала. Посему этим принебрегают.

Ниже, более эффективный вариант. Здесь вместо подмассивов сдвигается левая и правая
части массива. На каждой итерации цикла меняются местами только два числа, но зато
сдвиги частей позволяют пропустить множество чисел, которые уже отсортированы.
...
void quickSort(int* arr, int start, int end)
{
    if (start >= end) {
        return;
    }

    int pivot = rand() % end + start;
    
    int left = start;
    int right = end;

    while (left <= right) {
        while (arr[left] < pivot) left++;
        while (arr[right] > pivot) right--;

        if (left <= right) {
            int temp = arr[left];
            arr[left] = arr[right];
            arr[right] = temp;

            left++;
            right--;
        }
    }

    quickSort(arr, start, right);
    quickSort(arr, left, end);
}

-----------------------

ПОИСКИ ПУТЕЙ (НА ГРАФАХ)

Поиск в ширину (BFS): O(V + E). Находит самый короткий путь, для каждого узла в графе. 
V - это кол-во узлов (vertex), а E - кол-во ребер (edge).

Основная идея, это по порядку добавлять все узлы в очередь и проверять их. 
Узлы первого уровня должны идти в очереди раньше, чем узлы второго уровня и т.д.

Результатом должен быть массив, где индексы будут представлять узлы, а значения -
числа, обозначающие минимальное количество ребер до этих узлов (от начала).

Поиск в ширину хорошо подходит для таких задач, как поиск общих друзей или 
реализация сборки мусора.

vector<int> bfs(vector<vector<int>>& graph, int start)
{
    vector<int> res(graph.size(), -1);
    res[start] = 0;

    queue<int> q;
    q.push(start);

    while (!q.empty()) {
        int v = q.front();
        q.pop();

        for (int u : graph[v]) {
            if (res[u] == -1) {
                res[u] = res[v] + 1;
                q.push(u);
            }
        }
    }

    return res;
}

Если граф представлен не списком смежности, а матрицей смежности, то сложность
алгоритма увеличится до: O(V ^ 2). И он немного видоизменится.

Чтобы найти путь до конкретного узла, нужно также добавить вектор from, 
который будет хранить "v" из каждой итерации:
...
vector<int> from(graph.size(), -1);
...
if (int u : graph[v]) {
    ...
    from[u] = v;
}

А для восстановления пути - использовать ф-цию getPath():
...
vector<int> getPath(vector<int>& from, int node) 
{
    vector<int> path;

    for (int v = node; v != -1; v = from[v])
        path.push_back(v);
    reverse(path.begin(), path.end());

    return path;
}

Алгоритм Дейкстры: O(V ^ 2 + E). Находит самый быстрый путь, для каждого узла в графе. 
При этом, самый быстрый путь может быть и не самым коротким. 

Основная идея, это найти минимальный вес для каждого узла, относительно начального.
На каждой итерации цикла определяется расстояние до одной из вершин.

Внутри цикла сначала нужно найти ближайшую необработанную вершину. Для этого надо
рассматривать лишь те веришины, для которых в visitedNodes задано false, иначе 
будем всегда попадать на началную вершину.

Затем пометить ее как пройденную и проверить есть ли в результирующем массиве для нее
путь. Если он является более длинным, то обновить текущим значением.

vector<int> dijkstra(vector<vector<Edge>>& graph, int start)
{
    int gSize = graph.size();

    vector<int> res(gSize, INF);
    res[start] = 0;

    vector<bool> visitedNodes(gSize);

    for (int i = 0; i < gSize; i++) {
        int nearest = findNearest(graph, visitedNodes, res);
        visitedNodes[nearest] = true;

        for (auto& edge : graph[nearest]) {
            if (res[edge.to] > res[nearest] + edge.weight) {
                res[edge.to] = res[nearest] + edge.weight;
            }
        }
    }

    return res;
}

А так находится ближайшая необработанная вершина:
...
int findNearest(vector<vector<Edge>>& graph,
                vector<bool>& visitedNodes, vector<int>& res)
{
    int value = -1;

    for (int i = 0; i < graph.size(); i++) {
        if (!visitedNodes[i] && (value == -1 || res[value] > res[i])) {
            value = i;
        }
    }

    return value;
}

Восстановление пути делается также, как и в BFS. Но вместо "v", доб. "nearest". 
...
if (res[edge.to] > res[nearest] + edge.weight) {
    res[edge.to] = res[nearest] + edge.weight;
    from[edge.to] = nearest;
}

Поиск в ширину находит кратчайший путь в невзвешанном графе, а этот, наоборот, во
взвешанном. Алгоритм Дейстры не работает с графами, где узлы имеют отрицательный вес.
Он работает только с направленными, ацикличными графами.

-----------------------

ЖАДНЫЕ АЛГОРИТМЫ

Они позволяют находить приближенные решения для np-полных задач. Такие задачи 
не имеют быстрого алгоритмического решения и обычно выполняются за время: O(n!).

Признаки, по которым можно определить np-полную задачу:
* алгоритм быстро работает при малом кол-ве эл-тов, но сильно замедляется при
увеличении их числа;
* если нужно найти все возможные комбинации;
* если задачу можно переформулировать в условиях задачи покрытия множества 
или задачи о коммивояжере.

Количество всех возможных комбинаций - это факториал. Например для строки из 3х
символов, уникальных кобминаций будет 3! ("abc", "acb", "bac", "bca", "cab", "cba").

Жадные алгоритмы могут реализовываться по разному, это чисто творческое решение. 
Их основная цель - стремиться к локальной оптимизации в расчете на то, 
что в итоге будет достигнутнуто наиболее оптимальное решение.

Эффективность жадных алгоритмов оценивается по быстроте и близости полученого решения.
Они должны выполняться значительно быстрее, чем точные, например за: O(n ^ 2).

Задача о рюкзаке. Есть рюкзак, который может хранить до 4кг веса и есть 4 предмета:
магнитофон (4кг/$3000), ноутбук (3кг/$2000), гитара (1кг/$1500) и телефон (1кг/$1000).
Нужно заполнить рюкзаг наиболее выгодно. Каждый предмет можно взять только один раз.

Точный алгоритм даст правильный результат (ноут + гитара), но для этого придется
выполнить n! операций, перебрав между собой все возможные варианты. 

Жадный алгоритм даст приближенный результат, например один магнитофон или 
ноут + гитара, или ноут + телефон.

Суть всех жадных алгоритмов, это сначала взять самое дорогое (лучшее), после чего
заполнить свободное место тем, что осталось, в порядке стоимости.

-----------------------

ДИНАМИЧЕСКОЕ ПРОГРАММИРОВАНИЕ

Это подход к решению задач, при котором исходная задача разбивается на более мелкие
подзадачи. А их результаты позволяют найти общее решение (напоминает рекурсию).

ДП подходит для оптимизации np-полных задач. При этом, оно подходит не для всех задач 
и к таким решениям, порой, очень сложно додуматься.

Его можно применить, например, когда необходимо найти различные варианты комбинаций
(задача о коммивояжере, задача о рюкзаке, и т.п), либо для нахождения числа Фибоначчи.

Задача о рюкзаке сводится к перебору всех возможных множеств предметов и нахождению
самого оптимального. При большом кол-ве предметов, это займет много времени: O(2 ^ n).
Применив ДП, решение задачи можно оптимизировать.

Каждый алгоритм ДП начинается с таблицы. Для задачи о рюкзаке она будет такой:

   |    1     |    2     |    3     |    4
-----------------------------------------------
 Г | Г (1500) | Г (1500) | Г (1500) | Г (1500) 
-----------------------------------------------
 М | Г (1500) | Г (1500) | Г (1500) | М (3000)
-----------------------------------------------
 Н | Г (1500) | Г (1500) | Н (2000) | Н + Г (3500)

Здесь строки - это предметы, а столбцы - емкость рюкзака от 1 до 4кг.

В первой строке проверяется только гитара, и она подходит для любой ячейки.

Во второй, магнитофон и гитара. Магнитофон не помещается в первые 3 ячейки, и поэтому
туда записываются самые оптимальные решения, которые уже известны, т.е. гитары.

В третьей проверяются: ноут, магнитофон, гитара. Ноут подходит для третьей ячейки, 
но при этом остается еще 1кг пустого места. Его можно заполнить гитарой.

Таким образом находится самое оптимальное решение - 3500 (Н + Г). 
Телефон не брался в расчет, т.к. он весит как гитара, но стоит дешевле.

При оптимизации задач можно применять такие техники, как: мемоизации и табуляция.

Мемоизация - это техника, при котор. задача решается сверху вниз, т.е. через рекурсию. 
Здесь промежуточные вычисления обычно кешируются, если их нужно переиспользовать.

Например при вычислении четвертого числа Фибоначчи, можно запомнить второе число 
и не вычислять его каждый раз.

           Ф(4)
               ↙      ↘
      ф(3)        ф(2)
        ↙   ↘           ↙   ↘
   Ф(2)  Ф(1)  Ф(1)  Ф(1)
   ↙   ↘
Ф(1)  Ф(1)

Табуляция - это техника, при которой задача решается снизу вверх, т.е. через цикл. 
Здесь для решения более сложных подзадач, используются решения более простых подзадач.
...
for (int i = 3; i <= n; i++) {
    int c = a + b;
    a = b;
    b = c;
}

-----------------------

АЛГОРИТМ K-БЛИЖАЙШИХ СОСЕДЕЙ.

У него есть два основных применения:
1) Классификация (распределение предметов по категориям).
2) Регрессия (прогнозирование ответа в числовом виде).

Классификация определяется по кол-ву предметов, находящихся рядом. Каких соседей
больше, такого типа и будет предмет. Близость к соседям можно определять либо через
расстояния (по теореме Пифагора), либо через общие углы, с помощью косинусов.

На практике в основном используется близость косинусов.
Свойства, по которым определяется близость, называются признаками. 

Например нужно классифицировать фрукт. Признакими классификации будут размер и цвет,
определяемые по шкале от 1-5. В случае с цветом, 1 - это ораньжевый, а 5 - красный.

Тогда, при наличии признаков:
* 2х разных апельсинов: { {1, 2}, {3, 3} }
* одного грейпфрута: { 5, 4 }
* неизвестного фрукта: { 4, 3 }

Классифицируем фрукт, путем нахождения расстояний:
1) До первого апельсина: √(1 - 4)² + (2 - 3)² = 3.16
2) До вротого апельсина: √(3 - 4)² + (3 - 3)² = 1
3) До грейпфрута: √(5 - 4)² + (4 - 3)² = 2

Получается что ближайших соседей 2, это второй апельсин и грейпфрут, но посколько
апельсин ближе, то скорей всего неизвестный фрукт будем именно им.

Регрессия определяется также, путем нахождения общих соседий, и вычисления 
среднего арифметического между ними.

Допустим нужно спрогнозировать, сколько буханок хлеба выпекать на сегодня. 
Признаками будут: погода (1-5), выходной (0-1), акция (0-1).

При этом известно сколько хлеба продавалось ранее:
A) { 5, 1, 0 }  -->  300 буханок;
B) { 3, 1, 1 }  -->  225 буханок;
C) { 1, 1, 0 }  -->  75 буханок;
D) { 4, 0, 1 }  -->  200 буханок;
E) { 4, 0, 0 }  -->  150 буханок;
F) { 2, 0, 0 }  -->  50 буханок;

Сегодняшний день: { 4, 1, 0 }. А расстояния от него:
A = 1, B = 2, C = 9, D = 2, E = 1, F = 5

Дни: A, B, D, E являются ближайшими. Вычислив среднее арифметическое в эти дни, 
получим 218.75. Значит, именно столько буханок нужно выпекать.

Также, этот алгоритм применяется и в машинном обучении, например в технологии OCR
(оптическое распознавание текста), построении спам фильтров и т.д.

-----------------------

ДРУГИЕ АЛГОРИТМЫ

Вычисление факториала (произведение чисел от 1 до n):
...
int factorial(int n)
{
    return n > 1 ? n * factorial(n - 1) : 1;
}

Вычисление числа Фибоначчи (сумма двух предыдущих чисел, начиная с 2 и до n):
...
int fib(int n)
{
    return n > 1 ? fib(n - 1) + fib(n - 2) : n;
}

Нахождения минимального элемента в массиве:
...
int* minElement(int* begin, int* end)
{
    int* min = begin;
    for (int* it = begin; it != end; it++) {
        if (*it < *min) {
            min = it;
        }
    }

    return min;
}

-----------------------

РАЗНОЕ

Хорошее объяснение алгоритма Евклида (стр. 79).

Чтобы определить сложность алгоритма, нужно определить его скорость выполнения и объем
используемой памяти, например для поиска в ширину это O(V + E) и O(V) соответсвенно.
Подробнее здесь: https://habr.com/ru/articles/200252/

Бинарное дерево - это стркутура данных, в которой для каждого узла все узлы левого 
поддерева содержат меньшие значения, а все узлы правого поддерева - большие значения.

   D             Для поиска эл-тов используется бинарный поиск. Элементы в среднем   
↙      ↘                 находятся за время O(log n), в худшем случае - за O(n).
A       MA        
         ↙      ↘         Однако при удалении и вставке элементов, бинарное дерево работает     
     MG     MI     намного быстрее чем массив, за O(log n).

Бинарные деревья не поддерживают произвольный доступ к своим элементам, также их
элементы нельзя изменять (иначе нарушится логика построения соседних элементов).
Они обычно используются для хранения информации в базах данных.

Преобразование Фурье - это алгоритм, позволяющий разложить сущность на её состовляющие.
Например если есть блюдо, то преобразование Фурье может сообщить его ингридиенты.
Также, оно может разложить музыку на частоты, либо сжать изображение и пр.

Фильтры Блума применяются при работе с большими данными. Они дают приближенные ответы,
которые могут оказаться ложными, но с большой веройстностью являются правильными
(вместо обрабатки огромных коллекций для получения точных ответов).

Алгоритм SHA используется для генерации хеш-кода. Существует несколько версий SHA:
SHA-0, SHA-1, SHA-2, SHA-3. Версии SHA-0 и SHA-1 на данный момент уже слабые. 

Для каждой строки, SHA генерирует уникальный хеш-код (даже если строки отличаются
только одним символом). Если требуется локально-чувствительное хеширование, то можно
использовать алгоритм Simhash. При незначительном изменении строки, он генерирует 
почти идентичных хеш-код. Это полезно, для выявления сходства между фрагментами 
текста, например когда нужно проверить материал на плагиат/авторское право.

Хеш-код является односторонним, его нельзя расшифровать. Хеш-код всегда одинаковой
длины, независимо от того, генерируется он для маленькой строки или для большого
текста. По хеш-кодам часто проверяют подлинность файлов/паролей.

Алгоритм Диффи-Хеллмана применяется для шифрования сообщений. Он использует два ключа:
открытый и закрытый. Открытый ключ известен всем. Им необходимо шифровать сообщения, 
а для расшифровки использовать закрытый ключ. У этого алгоритма есть наследник - RSA.

Линейное программирование используется для максимизации некоторой характеристики при
заданных ограничениях, с пом. симплекс-метода. Например, можно максимизировать прибыль
при ограниченном кол-ве материалов, или голоса, при ограниченном бюджете.

