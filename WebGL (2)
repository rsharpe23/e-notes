ПРО МАТРИЦЫ

Матрицы в основном используются для взаимосвязи между двумя координатами.
Локальный вектор умноженный на матрицу, перемещается в глобальное пространство.

Определитель матрицы - это её число (результирующее).

Транспонирование матрицы - это операция, при которой строки становятся 
столбцами, а столбцы - строками.

Преобразования матриц бывают линейными и аффинными.

Линейные преобразования - это повороты и масшабирования. Они сохраняют состояние 
и могут приминяться в любой последовательности. Например если применить масштабирован,
а затем поворот, то результат будет таким же, как если бы сначала применить 
поворот, а затем масштабирование.

Преобразование считается линейным, если оно сохраняет базовые операции 
сложения и умножения на скаляр.

Аффинное преобразование - это линейное преобразование, за которым следует сдвиг.

Матрицы 3x3 используются для линейных преобразований и они не содержат сдвига.
Любое преобразование которое выполняется через матричное умножение не может содержать
сдвига. Для выполнения преобразований со сдвигом, нужно использовать матрицы 4x4.

Если объект необходимо двигать или поворачивать, то нужно преобразовать его вершины
(из локальных?) в экранные/мировые координаты с пом. матрицы модели (ModelView).

Чтобы преобразовать вершину, нужно умножить её на матрицу модели (с пом. векторного
произведения): Vf = Mv * V, где Mv - матрица модели 4х4, а V - вектор вершины. 
Каждая вершина должна определяться как: [x, y, z, w]. 

Матрица проекции преобразов. вершины относительно их расстояния к зрителю: 
Vp = P * V, где P - матрица проекции 4х4, V - вектор.

Параметры при установке матрицы проекции:

* fieldOfView - поле зрения; чем больше значение, тем больше объектов 
  будет охвачено на сцене;

* Near, Far - ближняя и дальняя плоскости для создания области отсечения;

Чтобы правильно отобразить вершину объекта на экране, нужно матрицу модели умножить 
на вектор вершины (перевести вектор в глобальное пространство), затем матрицу
проекции умножить на вектор вершины в глобальном пространстве.
...
vec4 gVertexPos = mvMatrix * vertexPos;
gl_Position = pMatrix * gVertexPos;

Чтобы получить матрицу модели, нужно умножить матрицу вида на мировую матрицу.
Мировая матрица - это матрица, содержащая трансформации объекта.

----------------------

ОСНОВЫ 3D-ГРАФИКИ

Модель в 3D-игре называется мешом. Каждая грань меша называется полигоном. 
Полигон состоит из 3х и более вершин. Каждая вершина обозначается местоположением 
на экране. Местоположение определяется с помощью векторов.

Каждой вершине, которая определяется, даётся числовая метка; например, вершина 0
помечается нулём, вершина 1 - единицей и т.д. Эти метки называются индексами.

Индексы информируют WebGL о том, как соединять вершины, чтобы 
сформировался полигон или поверхность.

Чтобы получить наиболее гладкую поверхность, с минимальным кол-вом полигонов, нужно
использовать полигоны с наименьшим кол-во сторон, т.е. треугольники. Если вместо
треугольников использовать квадраты, то их придется задать намного больше. Также 
у треугольников 2 из 3х сторон общие, тогда как у квадратов 2 из 4х.

------------------------

КОНВЕЙЕР ОТРИСОВКИ (RENDERING PIPELINE)

Старые видеокарты не позволяли напрямую изменять или манипулировать вершинами, 
но имели встроенный ф-ционал для их поворорота или масштабирования.

Современный конвейер отрисовки даёт полную гибкость в изменении вершины 
и отрисовки объектов с пом. шейдеров.

Этапы конвейера отрисовки (стр. 19):

  JavaScript > Вершинный шейдер > Сборка примитивов и Растеризация 
                                                V
  Фреймбуфер < Pre-fragment шейдер < Фрагментный шейдер

Графические ускоритель (graphics accelerator) - это аппаратное обеспечение 
(hardware), предназначенное для рисования графики. 

Он имеет область памяти для хранения содержимого отображения. Каждый видимый пиксель
представлен несколькими байтами памяти (display memory); Эта память обновляется
определенное кол-во раз в секунду для обеспечения отсутствия мерцания.

Граический ускоритель также предоставляет внеэкранную память (offscreen momory),
которая используется только для хранения данных.

Распределение памяти отображения и внеэкранной памяти управляется оконной системой.

Область графической памяти, которая изменяется в результате рендеринга называется
фреймбуфером. Фреймбуфер по умолчан. предоставляется оконной системой.

Если необходимо выполнить рендеринг в текстуру, то нужно создавать 
закадровый (внеэкранный) фреймбуфер.

--------------------

ОСНОВЫ WEBGL

Буферный объект предоставляет методы для загрузки атрибутов на видеокарту. Он даёт
существенный прирост производительности при рендеринге, поскольку его данные находятся
в видеопамяти: const bufferObj = gl.createBuffer();

Буферных объектов может быть много. Метод bindBuffer() вызывается, чтобы сообщить, 
на каком из объектов буфера будут работать последующие операции (WebGL).

Буферные объекты могут быть: вершинными (VBO - Vertex Buffer Object), 
индексными (IBO), текстурными (TBO) и пр.

Тип ELEMENT_ARRAY_BUFFER используется только для хранения индексов. Для остальных
данных, используется ARRAY_BUFFER (позиции, цвета, нормали и т.п.).

Метод bufferData() выделяет память для буферного объекта. Третий параметр указывает,
как его использовать. Обычно передается значение STATIC_DRAW.

Все значения 3го параметра:

* STATIC_DRAW - данные объекта указаны приложением один раз и используются
  много раз для рисования примитивов;

* DYNAMIC_DRAW - данные объекта указаны приложением неоднократно и используются 
  много раз для рисования примитивов; используется для анимации отдельных вершин;

* STREAM_DRAW - данные объекта указаны приложением один раз и используются 
  всего несколько раз для рисования примитивов;

Программа WebGL разделена на два компонента; управляющий код JS и шейдеры.
Управляющий код выполняется на CPU, а шейдеры на GPU. Поскольку управляющий код
связывает данные с памятью GPU, он доступен для обработки в шейдере.

Вершинный шейдер - это программируемый модуль, который работает с входящими значениями
вершин и связанными с ними данными (позиции вершин, их цвета, нормали и пр.).

Обычно в нём выполняются след. операции:
* вершинные преобразования;
* преобразования нормалей и нормализация;
* генерация текстурных координат и их преобразования;
* применение материала и освещения;

Данные, которые передаются в вершинный шейдер:
* атрибуты - поэтапные данные вершин, поставляемые из вершинных буферов.
* униформы - константные данные сразу для всего шейдера; вершинный и фрагментный 
  шейдеры имеют общее глобальное пространство для униформ.

gl_Position - это varying-переменная, которая объявляется автоматически.

После того, как вершинный шейдер отработает с атрибутами вершин, следующая фаза - это
сборка примитивов (Primitive Assembly). На этой фазе, если примитив лежит частично 
за пределами усеченной пирамиды вида (frustum view) то он обрезается, а если он 
вне её пределов, то полностью отсекается. 

После этого идёт фаза растеризации. На этой фазе все примитивы преобразуются в
двумерные фрагменты, также называемые пикселями. Эти фрагменты затем 
обрабатываются фрагментным шейдером.

Во фрагментном шейдере выполняются след.:
* операции над интерполированными значениями;
* доступ к текстурам;
* применение текстур;
* сумма цветов;
* туман;

Данные, которые передаются во фрагментный шейдер:

* varying-переменные - данные из вершинного шейдера, генерируемые при 
  растеризации для каждого фрагмента (с помощью интерполяции).

* униформы - константные данные сразу для всего шейдера.

* семплеры - это особые типы, используемые фрагментным шейдером, 
  которые представляют текстуры.

Атрибуты, объявленные в шейдере, инициализируются в основном управляющем коде.
Чтобы их инициализировать, необходимо получить на них ссылки через метод
getAttribLocation(). Он как бы запрашивает местоположение атрибута.

Атрибутами могут быть только вещественные числа, а также векторы 
или матрицы (с вещественными числами).

Чтобы получить ссылку на униформу, используется метод getUniformLocation(). 
А метод uniform*() загружает данные в униформу.

Метод vertexAttribPointer() сопоставляет текущий активный буфер с атрибутом шейдера.
Его 4й параметр (norm) это индикатор, указывающий, используются ли невещественные
данные, которые должны быть нормализованы, перед конвертацией во float.

Обычно он вызывается перед методом draw*() когда надо отрисовать более одного 
объекта на сцене (в связке с методом bindBuffer()).

Метод drawArrays() эффективен, когда нужно отрисовать простую геометрию.
Для сложной геометрии используется метод drawElements(). Он использует IBO и метод
drawArrays(), который в свою очередь использует VBO для чтения данных.

Этапы построения WebGL-приложений:
* создать контекст WebGL;
* загрузить, скомпилировать и линкануть шейдеры;
* инициализировать буферные объекты;
* применить преобразования и соединить буферные объект с атрибутами;
* вызвать drawArrays() или drawElements();

Управляющий код отлаживается по обычному, через console.log() и интструменты отладчика.
Отладка шейдеров происходит с помощью браузерного плагина - WebGL Inspector.

------------------------

НОРМАЛИ И ОСВЕЩЕНИЕ

Точность для типа float в вершинном шейдере может быть низкой, а во фрагметном -
высокой, поскольку вершинам, в основном, передаются конкертные значения, а пикселям - 
промежуточные (интерполируемые), требующие более точного рассчета.

Нормали также необходимо преобразовывать в мировое пространство; поскольку после
поворота или сдвига объекта они, как и вершины, поменяют свои значения. 

Чтобы преобразовать нормаль, нужно сначала найти транспонирование 
обратной ModelView-матрицы:
...
const nMatrix = mat3.create();
mat4.toInverseMat3(mvMatrix, nMatrix);  -->  Матрица нормалей - это обратная 
mat3.transpose(nMatrix);                     ModelView-матрица

gl.uniformMatrix3fv(u_NMatrix, false, nMatrix);

Затем, в шейдере, умножить сырую нормаль на транспонированную матрицу:
const tNormal = u_NMatrix * a_Normal;

Типы источников света:
* Направленный свет (directional lights) - солнечный свет;
* Точечный свет (point lights) - свет лампы;
* Прожектор (spotlights) - фонарь;

Окрущающее освещение (Ambient lighting) создает равномерный свет. Вычисляется по
формуле: I = Ia Ka, где Ia - интенсивность фонового света, Ka - коэффициент фонового
отражения объектов, от 0.0 до 1.0 для каждого RGB-компонента. Его свойства:

* генерируется направленным источником света;
* имитирует свет, который отражается от разных поверхностей;
* его интенсивность не меняется от положения полигонов в пространстве;
* позиция зрителя не важна;

Диффузное (рассеянное) отражение (Diffuse reflection) - это отражение света от
поверхности, при котором падающий луч отражается под многими углами. 

Вычисляется по формуле: I = Ip Kd (N'.L'), где Ip - интенсивность точечного света, 
Kd - коэффициент диффузного отражения объектов, от 0.0 до 1.0 для каждого RGB-компонен.
N' - нормализованная нормаль поверхности/вершины, L' нормализованное направление
источника света, точка - это скалярное произведение 2х векторов.

Свойства диффузного отражения:

* яркость полигона зависит от теты - угла между нормалью поверхности (N) 
  и направлением источника света (L);

* позиция зрителя зависит от того, используется ли направленный свет или точечный;

Материал - это описание поверхности объекта. Свойства материала описывают то, как он
отражает фоновый и диффузный свет, а также характеристики зеркального блеска.

Фоновое и диффузное отражения работают вместе, чтобы определить воспринимаемый цвет
объекта и обычно задаются одинаковым цветом. Например, если материал отражает только
зеленый цвет, фонового и диффузного света, то находясь в сцене с белым светом, 
объект будет зелёным. Но если свет станет синим, то объект почернеет, 
т.к. материал не отражает синего.

Зеркальное отражение задаёт блики на объектах, делая их блестящими. Оно определяется
двумя свойствами: цветом блика и общим блеском материала.

------------------

МОДЕЛИ ОСВЕЩЕНИЯ/ОТРАЖЕНИЯ

Алгоритмы, используемые для рассчета направления, интенсивности и цвета 
отраженного света называются моделями отражения.

Модели отражения используют цветовые компоненты (фоновый, диффузный и зеркальный)
источника света, а также материал объекта для вычисления цвета фрагмента.

Рендеринг изображений с реалистичной моделью отражения называется BRDF 
(Bidirectional Reflectance Distribution Functions). Распространенные модели BRDF:

* Модель Ламберта - самая простая модель; используется для рендеринга объектов 
  с диффузным материалом и рассеянным светом.

* Модель Фонга - модель, подобная пластиковой зеркальности; но она не работает
  когда угол между вектором обзора (V) и вектором отражения (R) больше 90 градусов.

* Модель Блинна-Фонга - модифицир. модель Фонга, которая допускает определенные
  интерполяции и снижает вычислительные затраты; используется для рендеринга 
  объектов с зеркальным материалом.

* Модель Торренса-Спарроу - общая модель, кот. представляет поверхности как
  распределения идеальных зекральных микрограней;

* Модель Кука-Торренса - модель Торренса-Спарроу, кот. учитывает длину волны 
  и, следовательно, изменение цвета.

* Анизитропная модель Уорда - модель зеркальных микрограней с 
  эллиптической ф-цией распределения по Гауссу.

В играх, в основном, используются модели Ламберта и Блинна-Фонга, 
т.к. они требуют наименьших вычислений.

------------------

МОДЕЛИ ЗАТЕНЕНИЯ/ИНТЕРПОЛЯЦИИ

Модель затенения определяет, будем ли мы вычислять освещение только для вершин 
или для каждого пикселя грани. 

Если вычислять освещение для вершин, то значения пикселей между вершинами линейно
интерполируются. При этом вычисления придётся делать в вершинном шейдере. Такая модель
называется затененим Гуро. Когда освещение рассчитывается во фрагментном шейдере, 
то это называют затенением Фонга.

Наиболее распространенные модели затенения:

* Плоское затенение (Flat shading) - используется для высокоскоростного рендеринга;

* Затенение Гуро (Gouraud shading) - сила и слабость этого затенения заключается 
  в его интерполяции; если полигон охватывает больше пикселей на экранне чем число
  собственных вершин, то значения интерполируемых цветов из дорогостоящих рассчетов
  освещения в вершинах менее загружают процессор, чем выполнение расчета для каждого
  пикселя; однако зеркальные блики могут отображаться неправильно;

* Затенение Фонга (Phong shading) - работает лучше, чем затенение Гуро, когда
  применяется к модели отражения с небольшими зеркальными бликами, например к Фонгу; 
  но при этом является более дорогостоющим.

Затенение Гуро для модели отражения Ламберта на стр. 75. 
Расчет освещения выполняется в вершинном шейдере.

Затенение Гуро для модели отражения Блинна-Фонга на стр. 80.
Расчет освещения выполняется в вершинном шейдере.

Затенение Фонга для модели отражения Блинна-Фонга на стр. 82.
Основной расчет освещения выполняется во фграгментном шейдере. 
Вершинный шейдер только подготавливает некоторые переменные.

------------------

НЕСКОЛЬКИХ ОБЪКТОВ И ИСТОЧНИКОВ СВЕТА

Каждая 3D-модель может быть представлена классом Geometry. Объект Geometry должен
содержать примерно такие данные: вершины, нормали, UV-координаты, материалы, индексы.

В последствии все эти данные должны передаваться соответсвующим буферам:
...
gl.bindBuffer(gl.ARRAY_BUFFER, vbo);
gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(geometry.vertices), ...);

gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, ibo);
gl.bufferData(gl.ELEMENT_ARRAY_BUFFER, new Uint16Array(geometry.indices), ...);

gl.bindBuffer(gl.ARRAY_BUFFER, tbo);
gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(geometry.uvs[materialIndex]), ...);

Класс Face представляет собой полигон 3D-модели.
Пример реализации классов Geometry и Face на стр. 86.

Класс StageObject содержит информацию для рендеринга геометрии на сцене. Он
инициализирует объект Geometry, а также буферные объекты и дефолтный материал для
Geometry. Также он сожержит местоположение и поворот объекта относительно сцены.

Класс Stage содержит массив объектов сцены - StageObject, а также 
инициализированный контекст WebGL (в переменной gl).

Пример реализации классов Stage и StageObject на стр. 92.

Чтобы отобразить несколько объектов на сцене, в главном коде нужно последовательно
проинициализировать все StageObject, предварительно связав их со Stage, 
и вызывать метод drawScene() для каждого объекта сцены. 
...
initGL(canvas)
initShaders();
initScene();

const stage = new Stage(gl);

const stageObject1 = new StageObject();
stage.addModel(stageObject1);
drawObject();

const stageObject2 = new StageObject();
stage.addModel(stageObject2);
drawObject();

Если для рендеринга всех объектов используется одна пара шейдеров, то униформы
материала и света, для каждого из объектов, должны устанавливаться 
внутри метода drawObject().

WebGL не выполняет рендеринг нескольких буферов в один момент времени. При загрузке
нескольких объектов у нас будет несколько буферов. Чтобы отрендерить все эти 
буферы, придется сначала активировать каждый из них по очереди.

Чтобы получить информацию об активном буфере и состоянии конвейера рендеринга, нужно
использовать следующие методы (использ. для ARRAY_BUFFER и ELEMENT_ARRAY_BUFFER):
...
gl.getParameter(gl.ARRAY_BUFFER_BINDING);  -->  получить ссылку на привязанный 
                                                на данный момент VBO

gl.getBufferParameter(gl.ARRAY_BUFFER, gl.BUFFER_SIZE);  -->  размер получ. буфера
gl.getBufferParameter(gl.ARRAY_BUFFER, gl.BUFFER_STATUS);  -->  статус получ. буфера

gl.isBuffer(...);

Перед тем, как трансформировать объект, нужно сохранить исходную матрицу mvMatrix 
в матричном стеке, а после рендерига объекта - восстановить её. Иначе преобразования
объектов будут относительно друг-друга, а не относительно сцены.
...
const mStack = [];

function pushMatrix() {
  const temp = mat4.create();
  mat4.copy(temp, mvMatrix);
  mStack.push(temp);
}

function popMatrix() {
  if (mStack.length > 0) {
    mvMatrix = mStack.pop();
  }  
}

function drawObject(index) {
  pushMatrix();
    mat4.translate(mvMatrix, mvMatrix, stage.stageObjects[index].location);
    mat4.rotateX(mvMatrix, mvMatrix, stage.stageObjects[index].rotationX);
    // Активация буферов и вызов drawElements(...)
  popMatrix();
}

У WebGL ограничено кол-во создаваемых униформ. Если привысить лимит, то будет ошибка
компиляции шейдеров. Чтобы запросить разрешенно кол-во униформ, использ. вызовы:
...
gl.getParameter(gl.MAX_VERTEX_UNIFORM_VECTORS);
gl.getParameter(gl.MAX_FRAGMENT_UNIFORM_VECTORS);

Это ограничение не позволяет реализовать множество источников освещения. 
Но чтобы его обойти, можно использовать массивы: 
...
const int LIGHTS_COUNT = 2;
uniform vec3 u_LightPositions[LIGHTS_COUNT];
varying vec3 u_LightRays[LIGHTS_COUNT];

for (int i = 0; i < LIGHTS_COUNT; i++) {
  vec4 lightPos = mvMatrix * vec4(u_LightPositions[i], 1.0);
  u_LightRays[i] = vertexPos - lightPos.xyz;
}

gl.uniform3fv(u_LightPositions, [4, 10, -10, -4, 10, -10]);  -->  массив 2х векторов

Окружающий (ambient) цвет не нужно рассчитывать в шейдерах, поскольку он не зависит 
от расстояния и ориентации объекта. Вместо этого можно суммировать его от всех
источников света и передать эту сумму в шейдер.

Пример реализации множественного освещения (с точечным светом) на стр. 110.
Отражение по Блинн-Фонгу. Затениние по Фонгу.

Для реализации источника света используется класс Light.js. Он хранит его цвета,
направление и позицию. Для реализации всего освещения в целом использ. класс Lights.

------------------

ТЕКСТУРИРОВАНИЕ

Существует 3 типа текстур: 2D-текстуры, 3D-текстуры и Cubemap-текстуры 
(текстуры кубической карты). 

3D-текстуры редко используюся в играх. С пом. них в основном создают объемные эффекты,
такие как реалистичный туман или световые лучи. 

2D-текстура - это двумерный массив данных, обычно изображение формата .jpg или .png.
Тексели представлены в форматах: gl.RGB, gl.RGBA, gl.LUMINANCE или gl.ALPHA.
Тексели индексируется текстурными координатами.

Концепция мапинга, когда вершина полигона сопоставляется с определенной текстурной
координатой, называется -  Texture Mapping или UV Mapping.

Каждая текстурная координата представлена парой (s, t) или (u, v) и называется
текселем. U/S - представляет горизонтальную ось, а V/T - вертикальную ось.

Чтобы определить оптимальный цвет пикселя, применяются фильтры. Самый простой алгоритм
фильтрации выбирает цвет из ближайшего пикселя (интерполяция ближайших соседей). 

Поскольку объект, к которому применяется текстура, может находится на произвольном
расстоянии и в произвольной ориентации от камеры, один пиксель на экране может не
соответствовать напрямую одному текселю. 

Например, если переместить объект ближе к камере, размер текселей станет больше, 
чем размер пикселей на экране и их придется масштабировать. Процесс масштаблирования
текстуры называется Texture Magnification.

Если отодвинуть объект от камеры, то каждый тексель, наоборот, станет меньше пикселя 
и поэтому один пиксель может покрывать сразу несколько текселей. В этом случае
соответсвующий цвет будет подобран на основе покрытых текселей путем 
минификации текстуры - Texture Minification.

Метод pixelStorei() устанавливает режимы хранения пикселей для readPixels() 
и раскпаковывает текстуры с пом. texImage2D() и texSubImage2D().

Метод bindTexture() подобен bindBuffer(). Он делает указаную текстуру текущей для
дальнейших операций, таких как: texImage2D(), texParameteri() и др.

Метод texImage2D() основной. Он загружает текстуру в память GPU:
* target - цель; либо это TEXTURE_2D, либо одна из граней кубической карты;
* level - указывает какой mipmap-уровень загружать;
* internalFormat - внутренний формат хранения текстур: RGBA, LUMINANCE, ALPHA и др;
* format - формат данных пикселей: GL_RED, GL_RGBA, GL_BRGA и др;
* type - тип входящих данных пикселей: UNSIGNED_BYTE, UNSIGNED_SHORT_4_4_4_4 и др;
* pixels - содержит актуальные данные пикселей для изображения;

Метод texParameteri() устанавливает параметр для текущей текстуры:
* target - цель; например: GL_TEXTURE_1D, GL_TEXTURE_2D, GL_TEXTURE_CUBE_MAP и др;
* pname - имя параметра текстуры: GL_TEXTURE_MAG_FILTER, GL_TEXTURE_MIN_FILTER и др;
* param - значение для параметра текстуры;

Чтобы загрузить текстуру на GPU, нужно сначала активировать её. Делается это 
с пом. метода activeTexture(), который принимает индекс текстуры. 

Всего можно активировать до 32х текстур, при отрисовке одного примитива.
В результате текстуру можно передать в униформу семплера по индексу.
...
gl.activeTexture(gl.TEXTURE0);   -->  активируем TEXTURE_2D с индексом TEXTURE0
gl.bindTexture(gl.TEXTURE_2D, texture);  -->  после активации обычно следует привязка
gl.uniform1i(u_Sampler, 0);  -->  передаем текстуру в семплер по её индексу

Индексы задаются в диапазоне, от TEXTURE0 до TEXTURE31. 

Из-за ограничения в 32 текстуры, обычно для каждого примитива создают одну большую
текстуру, которая размещает в себе все необходимые текстуры поменьше 
(кол-во маленьких текстур может превышать 32).

Подход, когда для одного примитива задается несколько текстур, называется
мультитекстурированием (стр. 158). Оно используется когда нужны какие-то эффекты.

Мультитекстурирование реализуется путём смешивания нескольких текстур между собой, 
для получения усредненных значений (цвета) текселей.

Сэмплер содержит ссылку на текстуру. Задаётся он типом: sample1D/2D/3D. Переменную
сэмплера нельзя инициализировать каким-то значением. Её можно только объявить. 

В ф-цию передаётся только как входной параметр (с ключевым словом in):
void myFunction(in sampler2d myTexture) { }

Процесс получения данных из текстуры называется сэмплингом. Ф-ция texture*()
возвращает тексель текстуры, по заданным координатам. Размерность координат зависит 
от размерности сэплера, например для sampler1D координаты представлены типом float, 
а для sampler2D - типом vec2.

Ф-ция textureOffset() добавляет тексельное смещение к текстурным координатам.
Это полезно при сэмплинге коллекции изображений, в одну текстуру.

Пример текстурирования куба на стр. 124.
Пример загрузки модели с текстурами на стр. 148.
Пример загрузки модели с одной большой текстурой на стр. 204.

Для сочетания текстурирования с освещением, нужно итоговый цвет, полученный через
освещение, умножить на цвет текселя из текстуры:
...
gl_FragColor = vec4(iColor, 1.0) * texture2D(u_Sampler, vTexCoord);

Важно знать, что в этом случае, при рассчете освещения не должен использоваться
диффузный цвет материала, т.е. пропускаем meterialDiffuseColor (стр. 208).
...
iColor = u_AmbientColor * materialAmbientColor + u_DirColor * dirLightWeighting +
  u_SpecularColor * materialSpecualColor * specular;

Mipmap'ы - это оптимизированная коллекция изображений главной текстуры. Они
используется для увеличения скорости рендеринга и уменьшения артефактов сглаживания.

Средство визуализации (рендерер) переключается на подходящее mipmap-изображение 
в зависимости от расстояния до объекта. Mipmapping позволяет свести к минимуму
появление различных артефактов при рендеринге.

Mipmap'ы генерируются автоматически с пом. метода: gl.generateMipmap().
При этом можно загружать серию изображений вручную.
...
gl.bindTexture(gl.TEXTURE_2D, texture);
gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, img);
gl.texImage2D(gl.TEXTURE_2D, 1, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, img256);
gl.texImage2D(gl.TEXTURE_2D, 2, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, img128);
gl.texImage2D(gl.TEXTURE_2D, 3, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, img64);

В примере выше загружается несколько mipmap-изображений для одного и того же 
буферного объекта с разными уровнями детализации (LOD).

Методы фильтрации текстур:

Интерполяция ближайших соседей - самый простой и быстрый метод фильтрации. В качестве
цвета пикселя он просто использует цвет текселя, ближайшего к центру пикселя. Однако
рендеринг текстур будет не очень хорошим. Появляется много артефактов, таких как
блочность при увеличении и сглаживание/мерцание при уменьшении.
...
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);

Линейная интерполяция (билинейная фильтрация) - метод, в котором выбираются четыре,
ближайших к центру пикселя, текселя, цвета которых комбинируются по средневзвешанному
значению их растояния. Это устраняет блочность, наблюдаемую при увеличении.
...
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);

Ближайшие соседи с mipmapping'ом - сначала выбирается наиболее подходящий уровень
mipmap, а затем тексель, ближайший к центру пикселя. Значительно уменьшает сглаживание
и мерцание, но не помогает с блочностью.
...
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST_MIPMAP_NEAREST);

Билинейная фильтрация с mipmapping'ом - четыре, ближайших к центру пикселя, текселей
выбираются на ближайшем mipmap-уровне. Цвета текселей комбинируются по средневзвешанн.
значению их расстояния. Устраняет блочность, наблюдаемую при увеличении. Таким образом,
вместо резкого изменения цвета теперь происходить плавный градиент изменеия цвета 
от одного текселя к другому.
...
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR_MIPMAP_NEAREST);

Трилинейная фильтрация - сначала выполняет поиск текстуры, а затем применяет билинейн.
фильтрацию (1) или фильтрацию ближайших соседей (2) на два ближайших mipmap-уровня.
После этого линейно интерполирует полученные значения. Это решает распространенную
проблему, возникающую в билинейно отфильтрованных mipmap-изображениях. Очень заметно
изменение качества, когда рендерер переключается с одного mipmap-уровня на другой. 
...
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR_MIPMAP_LINEAR);  (1)
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST_MIPMAP_LINEAR); (2)

Mipmapping и соответствующие ему методы фильтрации можно применять только к POT-
изображениям (высота и ширина равна степени двойки: 64х64, 256х128). К изображениям,
отличным от POT, можно применять только билинейную фильтрацию и ближайших соседей.

Кубические карты (cubemaps) - это текстуры, кот. используются в играх для аппроксимац.
отражения окружающей среды на поверхности модели. Это может быть, например, 
skylight-освещение или скайбоксы.

Кубическая карта состоит из шести двумерных изображений, соответсвующих граням куба.
Координаты текстуры определяются с помощью вектора направления. 

Вектор направления имеет след. компоненты: s, t, p. Он исходит из центра куба 
и указывает на текслель грани (стр. 157).

Пример реализации кубической карты (мультитекстурирование):
...
сonst cubeTexture = gl.createTexture();
gl.bindTexture(gl.TEXTURE_CUBE_MAP, cubeTexture);

gl.texParameteri(gl.TEXTURE_CUBE_MAP, gl.TEXTURE_MAG_FILTER, gl.NEAREST);
gl.texParameteri(gl.TEXTURE_CUBE_MAP, gl.TEXTURE_MIN_FILTER, gl.LINEAR);

gl.texImage2D(gl.TEXTURE_CUBE_MAP_POSITIVE_X,  
              0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, image1);

gl.texImage2D(gl.TEXTURE_CUBE_MAP_NEGATIVE_X, 
              0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, image2);

gl.texImage2D(gl.TEXTURE_CUBE_MAP_POSITIVE_Y,  
              0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, image3);

gl.texImage2D(gl.TEXTURE_CUBE_MAP_NEGATIVE_Y, 
              0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, image4);

gl.texImage2D(gl.TEXTURE_CUBE_MAP_POSITIVE_Z, 
              0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, image5);

gl.texImage2D(gl.TEXTURE_CUBE_MAP_NEGATIVE_Z, 
              0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, image6);

gl.activeTexture(gl.TEXTURE0);  
gl.bindTexture(gl.TEXTURE_2D, texture);
gl.uniform1i(u_Sampler, 0); 

gl.activeTexture(gl.TEXTURE1);
gl.bindTexture(gl.TEXTURE_2D, cubeTexture);
gl.uniform1i(u_CubeSampler, 1);

Применение кубической карты во фрагментном шейдере:
...
gl_FragColor = vec4(iColor, 1.0) * texture2D(u_Sampler, v_TexCoord) 
  * textureCube(u_CubeSampler, tNormal);  -->  смешиваем тексели TEXTURE0 и TEXTURE1

------------------

РАБОТА С КАМЕРОЙ

Для поворота камеры, нужно повернуть вектор: взгляда, верха или вправо.
Вращение вокруг вектора взгляда называется Roll. Вращение вокруг верха - Yaw. 
Вращение вокруг вектора вправо - Pitch (cм. связанная система координат самолета).
Вращение по этим векторам/осям задается через углы Эйлера.

Но такое вращение имеет недостаток. При повороте объекта, его оси поворачиваются 
вместе с ним. Если одну из осей повернуть на 90 градусов, то она станет параллельной
другой оси. В этом случае обе оси будут вращаться в одном и том же направлении. 
Это проблема называется - шарнирный замок (gimbal lock).

Чтобы избежать этого, используют кватернионы. Кватернион состоит из 4х компонентов: 
x, y, z, w. Компоненты xyz - это ось вращения, а w - величина поворота 
(если точнее, то w - это косинус половины угла поворота):
...
0, 1, 0, 45  -->  поворот вокруг Y на 45 градусов.

Помимо этого, вращение с использованием кватернионов происходит намного 
быстрее, чем при использовании матрицы поворота.

Кватернион можно получить из матрицы вращения и также получить матрицу 
из кватерниона: quat.fromMat3(); mat3.fromQuat(); 

Видимый объем камеры имеет форму усеченной пирамиды (frustum - пиримида со срезанным
кончиком). Объекты, находящиеся вне видимого объема, исключаются из рендеринга.
Видимый объем определяется ближней и дальней плоскостями пирамиды.

Угол обзора (field of view) - это угол между камерой и осью Y. Изменение угла приведет
к расширению или сжатию пирамиды, т.е. к уменьшение или увеличению объектов. 
Обычно задается в приделах от 45 до 60 градусов.

Соотношение сторон (aspect ratio) - это величина, определяющая форму пирамиды
(квадратная или прямоугольная пирамида). Определяется путем деления ширины основания 
на высоту основания. От этого зависит, будут ли объекты вытянутыми или зауженными.

В 3D-играх обычно используются камеры 3х типов: 
* свободная (free) - камера от первого лица, кот. перемещается в любом направлении;
* целевая (target) - камера от третьего лица, кот. следует за своей целью;
* орбитальная (orbit) - целевая камера, кот. вращается вокруг цели;

Класс Camera является базовым для всех камер (стр. 173).
Класс FreeCamera предоставляет ф-ционал свободной камеры (стр. 177).
Класс OrbitCamera предоставляет ф-ционал орбитальной камеры (стр. 191).

Реализация ф-ций вращения свободной камеры (через кватернионы):
...
const pitch = angle => rotateOnAxis(left, angle);
const yaw = angle => rotateOnAxis(up, angle);
const roll = angle => rotateOnAxis(dir, angle);

// Так задается поворот, относильно локальных координат
// Векторы dir, up, left вычисляются также, как в алгоритме  
// вычисления матрицы вида (стр. 179).

function rotateOnAxis(axis, angle) {
  const q = quat.create();
  quat.setAxisAngle(q, axis, angle);

  vec3.transformQuat(dir, dir, q);
  vec3.normalize(dir, dir);

  vec3.transformQuat(left, left, q);
  vec3.normalize(left, left);

  vec3.transformQuat(up, up, q);
  vec3.normalize(up, up);
}

Ф-ция moveForward() перемещает камеру в направлении взгляда, 
на расстояние distance:
...
function moveForward(distance) {
  const x = pos[0] - distance * dir[0];
  const y = pos[1] - distance * dir[1];
  const z = pos[2] - distance * dir[2];
  pos = vec3.fromValues(x, y, z);
}

Ф-ция update() заставляет целевую камеру перемещаться/вращаться 
вместе с игровой моделью:
...
function update(timeStep) {
  if (vec3.squaredLength(linVel) > 0.0) {
    vec3.scale(vel, vel, timeStep);
    vec3.add(pos, vel, pos);
  }

  if (vec3.squaredLength(angVel) > 0.0) {
    pitch(angVel[0] * timeStep);
    yaw(angVel[1] * timeStep);
    roll(angVel[2] * timeStep);
  }
}

Чтобу установить камеру, используется матричный метод lookAt(). 
Он, по сути, и генерирует матрицу вида:
...
const viewMat = mat4.create();
const lookAtPos = vec3.create();
vec3.add(lookAtPos, pos, dir);
mat4.lookAt(viewMat, pos, lookAtPos, up);

Первый параметр (выходной) - это матрица вида. Второй параметр - позиция камеры. 
Третий - позиция взгляда. Четвёртый - вектор вверх (0, 1, 0).

Позиция взгляда (то, куда смотрит камера) вычисляется путём сложения 
позиции камеры с её направлением.

Алгоритм вычисления матрицы вида в lookAt():
...
Vector3 zAxis = normal(lookAtPos - pos);   -->  вектор взгляда
Vector3 xAxis = normal(cross(up, zAxis));  -->  вектор вправо
Vector3 yAxis = cross(zAxis, xAxis);       -->  вектор вверх

Matrix4 orientationMat = {       -->  матрица поворота?
  xAxis.x, yAxis.x, zAxis.x, 0,
  xAxis.y, yAxis.y, zAxis.y, 0,
  xAxis.z, yAxis.z, zAxis.z, 0,
  0, 0, 0, 1
};

Matrix4 translationMat = {
  1, 0, 0, 0
  0, 1, 0, 0,
  0, 0, 1, 0,
  -pos.x, -pos.y, -pos.z, 1
};

viewMat = translationMat * orientationMat;

Орбитальная камера - это та, что вращается вокруг цели и постоянно смотрит на неё.
Также, её можно понимать как камеру, которая движется по поверхности сферы.

Следовательно, её местоположение определяется тремя параметрами: азимутом, 
высотой (elevation) и радиусом (стр. 190).

Азимут - это угол горизонтального поворота, а высота - вертикального.

В играх, азимут можно заменить термином рыскание (yaw), высоту - тангажом (pitch),
а радиус - расстоянием. Кроме того, рыскание и тангаж, в отличии от азимута 
и высоты, используют дополнительные углы.

Простейшая реализация вращения орбитальной камеры:
...
const q = quat.create();
quat.fromEuler(q, 10, 45, 0);

const pos = vec3.fromValues(0, 0, -distance);
vec3.transformQuat(pos, pos, rot);

Расположение объекта относительно камеры от первого лица (стр. 218):
...
const mMatrix = mat4.clone(camera.viewMatrix);
mat4.invert(mMatrix, mMatrix);

mat4.multiply(mvMatrix, mvMatrix, mMatrix);  -->  объект будет в центре камеры.

Чтобы немного сместить объект в сторону (например это может быть рука с оружием), 
нужно перед умножением на матрицу mvMatrix, добавить трансформацию сдвига: 
...
mat4.translate(mMatrix, mMatrix, vec3.fromValues(1, -3, -8)); 

------------------

УПРАВЛЕНИЕ

При нажатии клавиши можно вращать камеру, умножая Пи на некий коэффициент:
camera.roll(-Math.PI * 0.025);

Чтобы управлять мышью, нужно рассчитывать угол поворота на основе расстояния,
пройденного при перетаскивании.

Класс KeyBoardInteractor задаёт управление камерой, через обработку клавишь (стр. 185).
Класс MouseInteractor задаёт управление камерой с помощью мыши (стр. 187).

------------------

АНИМАЦИЯ

В разработке игр наиболее распространенными анимациями являются: time-based анимация,
tween-анимация (интерполяция) и скелетная анимация (с использ. rigged-моделей).

Frame-based анимация - это анимация трансформаций объекта. Выполняется в каждом кадре,
добавляя к некой величине постоянное значение. Её скорость зависит от FPS, т.е. на
производительных компьютерах она выполняется быстрее.

Time-based анимация похожа на framed-based, но изменение величины зависит от дельты
времени между кадрами. Выполняется с одинаковой скоростью на любых компьютерах.

Tween-анимация - это анимация значений, которые рассчитываются через интерполяцию.
Обычно указывается начальное и конечное значения (и возможно некоторые промежуточные)
после чего, с помощью ф-ции, вычисляются все промежуточные значения.

Линейная интерполяция (lerp) - это метод аппроксимации кривой с использованием линейных
полиномов. Например, для 2х точек она определит промежуточные точки, которые будут
лежать вдоль прямой линии, от начальной, до конечной (стр. 215).

Spline-интерполяция - это интерполяция на основе сплайна (кривой). Предпочтительнее
линейной, т.к. сводить ошибки интерполяции к минимуму. Использ., когда ф-ция изменяет
значение x, например: если x > 0, то f(x) = x^2, а если x < 0, то f(x) = 1/x^2.

Самая распространенная форма интерполяции - это B-splain интерполяция. Она 
применяется, когда необходимо создать изогнутый путь (например траекторию снаряда). 
Для определения формы кривой используются контрольные точки (стр. 216).
...
const controlPoints = [[-1.5, -2, -10], [-1.5, 10, -30], [-1.5, 15, -40]];
const interpolation = BSplineInterpolation(controlPoints);
const projectilePositions = interpolation.positions;  -->  множество плавных позиций

Скелетная анимация - это анимация модели, которая представлена двумя частями: 
мешом (вершинами) и иерархическим набором взаимосвязанных костей (называется скелет 
или rig), используемым для движения меша. Иерархический набор означает, что при
перемещении родительской кости, перемещаются и дочерние.

Данная анимация применяется, в основном, для органических моделей. Но для удобства
может использоваться везде, где надо управлять деформацией объекта: двери, 
ложки, здания, и пр.

Инверсная кинематика - это техника, в которой используются уравнения кинематики 
для определения параметров сустава/соединения, обеспечивающие желаемое 
положение дочерних костей.  

Подробно о скелетной анимации рассказывается в главе 8.

------------------

ПУЛЯ, ГРАНАТА И ВЗРЫВ

Чтобы задать траекторию пули относительно сцены, нужно локальный вектор перемещения
умножить на матрицу (трансформации) камеры. Уравнение перемещения имеет вид: 
f(x) = z; т.е. изменяться должно только значение z (вектора).

Эффект взрыва создается с помощью физики частиц, но для простоты можно использовать
текстурную анимацию (в каждом кадре, через bindTexture(), привязывается новая текстура).

Пример реализации пули на стр. 223.
Пример реализации гранаты на стр. 230.
Пример реализации взрыва на стр. 234.

------------------

ФИЗИКА И ЛАНДШАФТ

Ладшафт - это плоскость состоящая из сегментов (более маленьких плоскостей).
Его проще всего создавать процедурно, через код. Для этого необходима карта высот, 
а также такие параметры, как: размер плоскости и кол-во сегментов.

Карта высот - это изображение, состоящее из оттенков серого. 
Темные области обозначают высоты, а светлые - низины.

Каждый сегмент обычно состоит из 2х треугольников.
Вершины сегмента вычисляются следующим образом:
...
const segmentWidth = width / gridX;    -->  ширина и высота одного сегмента
const segmentHeigth - height / gridZ; 

const x = j * segmentWidth - widthHalf;
const y = i * segmentHeight - heightHalf;

vertices.push(x);
vertices.push(-y);
vertices.push(0);

Чтобы ландшафт не был плоским, вершинам сегментов нужно задать высоту по коорд. Z
(вместо нуля). Вычисляется она с пом. карты высот (стр 275).
...
const height = minHeight + (colorValue / (maxColor - minColor)) 
  * (maxHeight - minHeight);

Вершинные индексы сегмента можно обозначить через (a, b, c, d):
...
const a = j + gridXN * i;              -->  0 + 4 * 0 = 0
const b = j + gridXN * (i + 1);        -->  0 + 4 * (0 + 1) = 4 
const c = (j + 1) + gridXN * (i + 1);  -->  (0 + 1) + 4 * (0 + 1) = 5
const d = (j + 1) + gridXN * i;        -->  (0 + 1) + 4 * 0 = 1

 0         1         2         3
 *---------*---------*---------*       i - индекс высоты сегментов (от 0 до gridZN)
 |         |         |         |       j - индекс ширины сегментов (от 0 до gridXN)
 |         |         |         |
 |  (0,0)  |  (0,1)  |  (0,2)  |       gridXN - ширина сегментов + 1
 |         |         |         |       gridZN - высота сегментов + 1
 *---------*---------*---------*
 4         5         6         7       gridX - ширина сегментов 
 |         |         |         |       gridZ - высота сегментов
 |  (1,0)  |  (1,1)  |  (1,2)  |       
 |         |         |         |       под шириной/высотой сегментов возможно  
 *---------*---------*---------*       подразумевается их кол-во по ширине/высоте
 8         9        10        11

UV-координаты сегмента вычисляются так:
...
const uvs = [[]];

const uva = vec2.fromValues(j / gridX, 1 - i / gridZ);
const uvb = vec2.fromValues(j / gridX, 1 - (i + 1) / gridZ);
const uvc = vec2.fromValues((j + 1) / gridX, 1 - (i + 1) / gridZ);
const uvd = vec2.fromValues((j + 1) / gridX, 1 - i / gridZ);

uvs[0].push(uva[0]);
uvs[0].push(uva[1]);
uvs[0].push(uvb[0]);
uvs[0].push(uvb[1]);
uvs[0].push(uvc[0]);
uvs[0].push(uvc[1]);
uvs[0].push(uvd[0]);
uvs[0].push(uvd[1]);

Класс PlaneGeometry генерирует все необход. данные плоской геометрии (стр 242).
Он хранит массивы: вершин, индексов, нормалей и uvs плоскости.

Класс Plane инициализирует объект PlaneGeometry, передавая след. параметры: 
width (ширина плоскости), height, widthOfSegments (ширина сегментов), heightOfSegments.

Популярные физические движки в JS (порты из C++):

* Ammo.js - порт движка Bullet. Полнофункциональный физический движок.
  Мощный, но не оптимизирован для работы в браузере. Плохая производительность. 
  Api не очень понятный. Bullet используется в GTA IV, Red Dead Redemption и др. 
  
* Box2dweb - порт движка Box2D. Имеет самую мощную constraint систему  
  (используется для симуляции ragdoll'ов, верёвок и т.п.) Легко настраивается.
  Работает только с 2D. Хорошая производительность.

* JigLibJS (устарел) - порт движка JigLib. Написан на JS. Хорошо оптимизирован
  для работы в браузере, однако не так многофункционален, как Ammo.js или Box2Dweb.

(На текущее время возможно уже что-то поменялось, м.б. появились новые движки).

Физический движок предоставляет след. функциональность:
* перемещение объектов в соотв. с набором физических правил;
* проверка столкновений между объектами;
* реакция на столкновения между объектами;

Физический движок для игр основан на классической (ньютоновской) механике, т.е. на 
трёх простых правилах движения (кот. изучаются в школе). Эти правила используются для
построения дифференциальных уравнений, тем самым описывая движение объектов. 

Затем дифференциальные уравнения решаются итеративно с пом. алгоритмов, используемых 
в движках, что приводит к правдоподобным движениям и взаимодействию эл-тов сцены.

Физический движок просматривает каждый физический объект в отдельном цикле симуляции,
проверяя его на столкновения, реагируя на них, а также изменяя некоторые св-ва
(сорость, положение и т.п.).

Симуляция физики работает при более высокой частоте кадров, чем основной рендеринг.
Подробно о том, как работает симуляция физики в JigLib на стр. 252. 

Большое кол-во физических объектов, могут занять много вычислительных ресурсов. 
Посему, нужно стараться чтобы их было как можно меньше. В качестве оптимизации можно
удалять объеты из системы, когда они не видны пользователю, и добавлять обратно
незадолго до того, как станут видны.
...
system.addBody(plane.rigidBody);
system.removeBody(plane.rigidBody);

Физический объект характеризуется физическим представлением, которое описывает 
его положение, ориентацию и движение.

Чем проще физическое представление, тем дешевле обходятся вычислительные затраты 
и, следовательно, тем большее кол-во объектов можно симулировать.

Основные типы представлений (симуляций):

* частицы - объекты, состоящие из пространственных точек (частиц); частица может
  двигаться в пространстве (т.е. имеет скорость), но не вращается и не имеет объема;
  используются для создания таких эффектов, как врывы, дым и т.п.;

* твёрдые тела (rigid bodies) - объекты, состоящие из пространсвен. форм (куб, сфера);
  могут двигаться и вращаться в пространстве, поэтому имеют как линейную, так и угловую
  скорость, а также объем; не могут деформироваться, поэтому такое название.

* мягкие тела (soft bodies) - деформированные объекты; имитирует все аспекты твёрдого
  тела (линейная и угловая скорость, объём), но с доп. особенностью изменения формы;
  используется для таких предметов, как одежда, волосы и т.п.; вычисления 
  значительно дороже, чем для твёрдого тела;

Твёрдые тела, которые не движутся, являются статическими, а те, что могут двигаться -
динамическими; Чтобы определить статическое твёрдое тело, используется вызов
set_movable(false). Ландшафты - лучший пример статических твёрдых тел.

Использование простых физических форм (коллайдеров), при рассчете столкновений,
значительно ускорит вычисления. В JigLib доступны следующие формы: JBox, JSphere,
JCapsule, JPlain, JTerrain и JTriangleMesh.

Форма JTriangleMesh создаёт коллайдер на основе меша. Она требует больших
вычислительных затрат, посколько проверяет на столкновения каждый полигон меша.

Если длина массива коллизий больше 0, это означает, что произошло столкновение.
Пример реализации физического объекта с пом. JigLib (стр. 256).
...
const system = jigLib.PhysicsSystem.getInstance();

system.setCollisionSystem();   -->  обнаружение столкновений (грубые столкновения)
system.setSolverType('FAST');  -->  реакция на столкновения (тип FAST - означает самое
                                                 быстрое дифференциальное уравнение)
const rigidBody = new jigLib.JSphere(null, 20);
rigidBody.set_mass(50);
rigidBody.moveTo(jigLib.Vector3DUtil.create(0, 100, 120));
system.addBody(rigidBody);

function update(elapsedTime) {
  system.integrate(elapsedTime / 1000);  -->  запуск симуляции

  const pos = rigidBody.get_currentState().position;
  mat4.identity(modelMatrix);
  mat4.translate(modelMatrix, modelMatrix, pos);

  requestAnimationFrame(update);
}

------------------

RAYCAST И ФРЕЙМБУФЕР

Raycast - луч, определяющий пересекаемые физические объекты. С пом. него можно
предсказать столкновение/попадание до того, как оно произойдет. 

Для создания raycast'а используются 2 класса: JRay и JSegment (библиотеки JigLib).

Конструктор JRay принимает 2 параметра: положение луча и направление.
JSegment аналогичен JRay, только вместо направления он принимает дельту. 

Дельта - это длина луча, умноженная на направление игрока. JSegment не создаёт
бесконечный луч. Он возвращает пересекающиеся объекты вблизи луча.

Также raycast используется для интерактивного взаимодействия пользователя с 
объектами на сцене, с помощью мыши (называется Picking).

Если объект сцены имеет простой коллайдер, формы сферы, куба или капсулы, то выбрать
можно только полную фигуру. Если объект имеет меш-коллайдер, то выбрать можно 
отдельный полигон объекта (куда попадёт луч/сегмент).

Чтобы получить точный треугольник, используется метод segmentIntersect(), 
объекта JTriangleMesh (работает медленно).

Пример реализация picking'а через raycast, на стр. 325-338 (сам луч выпускается 
с пом. метода pickObject(), реализация на стр. 335).

Чтобы перевести экранные координаты в мировые, нужно сначала перевести их в
нормализованные координаты устройства (NDC). Они не зависят от размера представления
(возможно имеется ввиду viewport) и варьируются от -1 до 1.

NDC-векторы вычисляются так (clientX/Y - координаты клика по канвасу):
...
const x = (clientX / screenWidth) * 2 - 1;
const y = (clientY / screenHeight) * 2 - 1;

Чтобы перевести NDC-вектор в мировые координаты, используется формула: 
v = M^-1 * P^-1 * v', где v' - это NDC-вектор, кот. выглядит так: (x, y, 0).
Эта операция называется - unproject vector (стр. 332).

А чтобы наоборот, получить экранные координаты, нужно выполнить: v' = P * M * v
(вершина умножается на ModelView-матрицу и на матрицу проекции камеры). Компоненты 
x и y, вектора v', обозначают местоположение на экране, а z - тест глубины.

Матрицы ModelView и Projection имеют размерность 4х4. Следовательно, чтобы умножать 
их на векторы v и v', те должны иметь доп. 4й компонент - w (со значением 1).

После умножении матриц на вектор, компонент w может не быть равен 1. Чтобы его
нормализовать, нужно выполнить разделение перспективы (projection divide). 
Выполняется оно путём деления всех координат на w (в applyProjection()).

function unprojectVector(vect, mvMatrix, pMatrix) {

  const viewProjMatrix = mat4.create();
  mat4.invert(viewProjMatrix, mvMatrix);

  const inverseProjMatrix = mat4.create();
  mat4.invert(inverseProjMatrix, pMatrix);

  // Результат M^-1 * P^-1 называется ViewProjection
  mat4.mul(viewProjMatrix, viewProjMatrix, inverseProjMatrix);

  return applyProjection(vect, viewProjMatrix);
}

function applyProjection(vect, m) {
  const [x, y, z] = vect;
  const d = 1 / (m[3] * x + m[7] * y + m[11] * z + m[15]);  -->  Perspective divide

  return vec3.fromValues(
    (m[0] * x + m[4] * y + m[8]  * z + m[12]) * d,
    (m[1] * x + m[5] * y + m[9]  * z + m[13]) * d,
    (m[2] * x + m[6] * y + m[10] * z + m[14]) * d
  );
}

Picking можно реализовать и с помощью фреймбуфера (на основе цвета объекта 
в позиции клика). Подробнее на стр. 322 и 338.

Фреймбуфер используется для реализации внеэкранного (offscreen) рендеринга. 
Это полезно, когда нужно что-то отрисовать, при том чтобы пользователь этого не видел.
(как раз то, что нужно для реализации системы брони танков). 

С пом. фреймбуфера можно также создавать тени, фильтры постобработки (размытие,
свечение, снег, ночное видение и пр.), кешировать изображения и т.д. Постобработку
можно делать и через одни только шейдеры, но это будет сложнее.

Объект фреймбуфера (FBO) позволяет игре захватывать сцены в память GPU для 
дальнейшей обработки. После захвата (отрисовки) можно считывать данные пикселей.

При работе с текстурами (в контексте фреймбуфера) цветовые данные можно хранить 
в объекте texture, а значения глубины - в объекте рендербуфера (RBO):
...
const texture = gl.createTexture();
gl.bindTexture(gl.TEXTURE_2D, texture);
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.NEAREST);
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);
gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, width, height,
              0, gl.RGBA, gl.UNSIGNED_BYTE, null);

const rbo = gl.createRenderbuffer();
gl.bindRenderbuffer(gl.RENDERBUFFER, rbo);
gl.renderbufferStorage(gl.RENDERBUFFER, gl.DEPTH_COMPONENT16, width, height);

В методе texImage2D() последнему параметру передается null (вместо image) 
чтобы просто выделить память, т.к. texture создается только для хранения данных.

Метод renderbufferStorage() создаёт память, для хранения 16-битного компонента 
глубины текстуры. API WebGL использует объекты рендербуфера для изображений 
(возможно имеется ввиду для хранения объектов image).

Далее нужно связать текстуру и рендербуфер с фреймбуфером:
...
const fbo = gl.createFramebuffer();
gl.bindFramebuffer(gl.FRAMEBUFFER, fbo);

gl.framebufferTexture2D(
  gl.FRAMEBUFFER, gl.COLOR_ATTACHMENT0,  --> связываем текстуру
  gl.TEXTURE_2D, texture, 0
);

gl.framebufferRenderbuffer(
  gl.FRAMEBUFFER, gl.DEPTH_ATTACHMENT,  --> связываем рендербуфер
  gl.RENDERBUFFER, rbo
);

После этого можно отвязать все объекты:
...
gl.bindTexture(gl.TEXTURE_2D, null);
gl.bindRenderbuffer(gl.RENDERBUFFER, null);
gl.bindFramebuffer(gl.FRAMEBUFFER, null);

Отрисовка (рендеринг) во фреймбуфер и на экран:
...
gl.bindFramebuffer(gl.FRAMEBUFFER, fbo);
drawScene();                                -->  рендерим во фреймбуфер
gl.bindFramebuffer(gl.FRAMEBUFFER, null);
drawScene();                                -->  затем на экран

При обычном рендеринге, вся отрисовка происходит в display framebuffer'е, который
является активным буфером по умолчанию. Чтобы перенаправить рендеринг в свой
фреймбуфер, нужно просто привязать его перед отрисовкой.

А чтобы вернуть рендеринг на экран, нужно отвязать свой фреймбуфер. 
После этого активным снова станет display framebuffer.

Создание фильтра постобработки с пом. фреймбуфера на стр. 340-349.
(также объясняется, как использовать несколько шейдеров).

При разработке игр, обычно создается общий шейдер для рендеринга всех объектов.
Но в некоторых случаях могут потребоваться доп. шейдеры для рендеринга отдельных
объектов, например для рендеринга главного персонажа или добавления эффекта частиц.

------------------

РАЗНОЕ

Кол-во источников света и выбор алгоритма затенения влияют на производительность.
Нужно стараться чтобы кол-во источников света было минимальным, подменяя их 
запеканием теней и лайтмапами. А еще нужно избегать зеркальных отражений, 
т.к. они сильно нагружают GPU.

materialDiffuseColor - одно из названий для свойства материала.
lamberdTerm - член Ламберта: max(dot(normal, -lightDir), 0.0);
specular - член Блинна: pow(specAngle, 16.0);

Обозначение суффиксов у некоторых методов:
...
funcName[1234][fi][v]() - означает ф-цию/метод с 1,2,3,4-параметрами типа float 
или int; если в конце стоит символ v, то вместо чисел нужно передавать 
вектор с 1,2,3,4-размерностью (т.е. массив).

Обзор форматов .obj и .mtl на стр. 55 и 131.
Конвертация файлов в JSON-файл на стр. 57.

Отношения между различными данными (вершин, нормалей, uv-координат) 
закодированы в массиве Faces.

Количество uv-координат должно быть равно кол-ву вершин. Если эти количества не
совпадают, то нужно создать избыточные координаты, чтобы их уровнять.

Модели объектов, текстуры, шейдеры и пр. обычно подгружаются через Ajax.

Метод requestAnimationFrame() вызывается тогда, когда вкладка браузера находится 
в фокусе. Это экономит ресурсы GPU. С помощью него можно реализовать цикл рендеринга,
который будет выполнятся настолько быстро, насколько позволит браузер.

Что такое materialFile? (как-то связанно с текстурами и mapDiffuse).
Это свойство StageObject, которое содержит текстуру объекта.

Система координат WebGL начинается в левом нижнем углу.
Эффект, когда подсвечиваются ребра объекта, называется edge detection.
Для создания GUI используется Canvas 2D.

